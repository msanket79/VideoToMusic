{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- WARNING: CUDA devices not detected. This will cause the model to run very slow! -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F #relu,\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from utils.vevo_dataset import create_vevo_datasets\n",
    "from utils.device import get_device,use_cuda\n",
    "from utils.constants import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use cuda\n",
    "# use_cuda(True)\n",
    "\n",
    "# yeh fucntion unhone direct provide kara hai for generating the train,test and val set (it supports pytorch dataloader)\n",
    "train_dataset,val_dataset,test_dataset = create_vevo_datasets(\n",
    "    dataset_root=\"./dataset\",\n",
    "    max_seq_chord=300,\n",
    "    max_seq_video=300,\n",
    "    vis_models=\"2d/clip_l14p\",\n",
    "    emo_model=\"6c_l14p\",\n",
    "    split_ver=\"v1\",\n",
    "    random_seq=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inka joh dataset hai woh dictionary ke form me stored hai [\"feature_semantic,motion,emotion,note_denity,loudess \"]  \n",
    "# in saare features ko combine karke ek tensor banaya hai jisko RNN(lstm,bilstm,gru,bigru) me daalenge\n",
    "# shape of tensor [batch_size,300,sabhi features ko combine karke ek vector] here 300 is video length(sequence length)\n",
    "def batchToInput(batch):\n",
    "    # extracting features from dataset\n",
    "    feature_semantic_list = [] \n",
    "    for feature_semantic in batch[\"semanticList\"]:\n",
    "        # print(feature_semantic.shape)\n",
    "        feature_semantic_list.append( feature_semantic.to(get_device()) )\n",
    "\n",
    "    feature_scene_offset = batch[\"scene_offset\"].to(get_device())\n",
    "    feature_motion = batch[\"motion\"].to(get_device())\n",
    "    feature_emotion = batch[\"emotion\"].to(get_device())\n",
    "    feature_note_density = batch[\"note_density\"].to(get_device())\n",
    "    feature_loudness = batch[\"loudness\"].to(get_device())      \n",
    "\n",
    "    x = feature_semantic_list[0].float()\n",
    "    for i in range(1, len(feature_semantic_list)):\n",
    "        x = torch.cat( (x, feature_semantic_list[i].float()), dim=2)            \n",
    "    x = torch.cat([x, feature_scene_offset.unsqueeze(-1).float()], dim=2) \n",
    "    x = torch.cat([x, feature_motion.unsqueeze(-1).float()], dim=2).to(get_device()) \n",
    "    x = torch.cat([x, feature_emotion.float()], dim=2).to(get_device()) #(N,300,some of length of output vectors for each feature)\n",
    "\n",
    "    y = torch.cat((feature_note_density.unsqueeze(-1).float(), feature_loudness.unsqueeze(-1).float()), dim=2) #(N,300,2)\n",
    "    return x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abhi jab saari video features ko ek vector me daalenge toh uska size calculte kar raha hu in total_vf_dim\n",
    "\n",
    "# input size cal\n",
    "total_vf_dim=0\n",
    "for vf in train_dataset[0][\"semanticList\"]: #feature semantic me kayi saare features ho sakte hai toh sabhi features ka length add kar raha hu\n",
    "    total_vf_dim+=vf.shape[1]\n",
    "\n",
    "total_vf_dim+=1 #scene offset\n",
    "total_vf_dim+=1 #motion\n",
    "\n",
    "#emotion (yeh do type ka hai 5c(5 length emotion output) and 6c(6 length emotion output)  )\n",
    "total_vf_dim+=6 #for 6c \n",
    "\n",
    "# params initalization (yeh params maine apni side se daale hai tweak kar hidden_size,num_layers,batch_size,epochs,dropout)\n",
    "input_size=total_vf_dim\n",
    "num_layers=2\n",
    "hidden_size=128\n",
    "output_size=2\n",
    "seq_len=300\n",
    "batch_size=32\n",
    "epochs=15\n",
    "dropout=0.1\n",
    "model_name=\"BiGRU\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now creating the model\n",
    "# yeh lstm ka model hai isme input dete hai [batch,300,inputsize(776)]\n",
    "# our output milta hai [batch,300,2] (2---> note_density,loudness for every frame)\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size=128,output_size=2,num_layers=2,seq_len=300,dropout=0.1):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.seq_len=seq_len\n",
    "        self.output_size=output_size\n",
    "        self.dropout=dropout\n",
    "\n",
    "        self.lstm=nn.LSTM(input_size,hidden_size,num_layers,batch_first=True)\n",
    "        self.fc=nn.Linear(hidden_size,output_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=F.dropout(x,p=self.dropout,training=self.training)\n",
    "        h0=torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device=get_device())\n",
    "        c0=torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device=get_device())\n",
    "\n",
    "        out, _=self.lstm(x,(h0,c0))\n",
    "        out=self.fc(out)\n",
    "        return out\n",
    "    \n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size=128,output_size=2,num_layers=2,seq_len=300,dropout=0.1):\n",
    "        super(BiLSTM,self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.seq_len=seq_len\n",
    "        self.output_size=output_size\n",
    "        self.dropout=dropout\n",
    "\n",
    "        self.lstm=nn.LSTM(input_size,hidden_size,num_layers,batch_first=True,bidirectional=True)\n",
    "        self.fc=nn.Linear(2*hidden_size,output_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=F.dropout(x,p=self.dropout,training=self.training)\n",
    "        h0=torch.zeros(self.num_layers*2,x.size(0),self.hidden_size).to(device=get_device())\n",
    "        c0=torch.zeros(self.num_layers*2,x.size(0),self.hidden_size).to(device=get_device())\n",
    "\n",
    "        out, _=self.lstm(x,(h0,c0))\n",
    "        out=self.fc(out)\n",
    "        return out\n",
    "    \n",
    "class GRU(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size=128,output_size=2,num_layers=2,seq_len=300,dropout=0.1):\n",
    "        super(GRU,self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.seq_len=seq_len\n",
    "        self.output_size=output_size\n",
    "        self.dropout=dropout\n",
    "\n",
    "        # self.lstm=nn.GR(input_size,hidden_size,num_layers,batch_first=True,bidirectional=True)\n",
    "        self.gru=nn.GRU(input_size,hidden_size,num_layers,batch_first=True)\n",
    "        self.fc=nn.Linear(hidden_size,output_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=F.dropout(x,p=self.dropout,training=self.training)\n",
    "        h0=torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device=get_device())\n",
    "\n",
    "        out, _=self.gru(x,h0)\n",
    "        out=self.fc(out)\n",
    "        return out\n",
    "    \n",
    "class BiGRU(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size=128,output_size=2,num_layers=2,seq_len=300,dropout=0.1):\n",
    "        super(BiGRU,self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.seq_len=seq_len\n",
    "        self.output_size=output_size\n",
    "        self.dropout=dropout\n",
    "\n",
    "        # self.lstm=nn.GR(input_size,hidden_size,num_layers,batch_first=True,bidirectional=True)\n",
    "        self.gru=nn.GRU(input_size,hidden_size,num_layers,batch_first=True,bidirectional=True)\n",
    "        self.fc=nn.Linear(2*hidden_size,output_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=F.dropout(x,p=self.dropout,training=self.training)\n",
    "        h0=torch.zeros(2*self.num_layers,x.size(0),self.hidden_size).to(device=get_device())\n",
    "        out, _=self.gru(x,h0)\n",
    "        out=self.fc(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeh isme num_workers maine apne CPU ke liye 12 daala tha , tu change karle apne hisaab se for better through put\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,num_workers=12,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,num_workers=12,shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,num_workers=12,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model\n",
    "model=0\n",
    "if model_name==\"LSTM\":\n",
    "    model=LSTM(input_size,hidden_size,output_size,num_layers,seq_len,dropout)\n",
    "if model_name==\"BiLSTM\" :\n",
    "    model=BiLSTM(input_size,hidden_size,output_size,num_layers,seq_len,dropout)\n",
    "if model_name==\"BiGRU\" :\n",
    "    model=BiGRU(input_size,hidden_size,output_size,num_layers,seq_len,dropout)\n",
    "if model_name==\"GRU\" :\n",
    "    model=GRU(input_size,hidden_size,output_size,num_layers,seq_len,dropout)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unhone model me yahi loss and adam use kia hai iss model ke liye \n",
    "criterion=nn.MSELoss() \n",
    "optimizer=optim.Adam(model.parameters(),lr=1e-3,weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeh func ek epoch ke liye chalta hai aur model ko train kardeta hai\n",
    "# criterion-->mse loss hai\n",
    "# batchToInput input ke shape ko [batch,300,size of all features] kardeta hai\n",
    "def train_epoch(curr_epoch,train_loader,criterion,optimizer,losses):\n",
    "    for batch_no,batch in enumerate(train_loader):\n",
    "        data,targets=batchToInput(batch)\n",
    "        scores=model(data)\n",
    "        \n",
    "        loss=criterion.forward(scores,targets)\n",
    "        losses.append(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeh function loader leta hai aur phir avg_rms,avg_loss, avg_rmse_loudness,avg_rmse_note_Denisty return karta hai\n",
    "def eval_model(model,dataloader,loss):\n",
    "    model.eval()\n",
    "\n",
    "    avg_rmse=-1\n",
    "    avg_loss=-1\n",
    "    avg_rmse_note_density=-1\n",
    "    avg_rmse_loudness=-1\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        n_test=len(dataloader)\n",
    "\n",
    "        sum_loss=0.0\n",
    "        sum_rmse=0.0\n",
    "\n",
    "        sum_rmse_note_density=0.0\n",
    "        sum_rmse_loudness=0.0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            data,targets=batchToInput(batch)\n",
    "\n",
    "            scores=model(data)\n",
    "            scores=scores.reshape(scores.shape[0]*scores.shape[1],-1).float()\n",
    "            targets=targets.reshape(targets.shape[0]*targets.shape[1],-1).float()\n",
    "            mse=F.mse_loss(scores,targets)\n",
    "            rmse=torch.sqrt(mse)\n",
    "            sum_rmse+=float(rmse)\n",
    "\n",
    "            scores_note_density,scores_loudness=torch.split(scores,split_size_or_sections=1,dim=1)\n",
    "            targets_note_density,targets_loudness=torch.split(targets,split_size_or_sections=1,dim=1)\n",
    "\n",
    "            # calculting rmse note density\n",
    "            mse_note_density=F.mse_loss(scores_note_density,targets_note_density)\n",
    "            rmse_note_density=torch.sqrt(mse_note_density)\n",
    "            sum_rmse_note_density+=float(rmse_note_density)\n",
    "\n",
    "            # calculating rmse loudness\n",
    "            mse_loudness=F.mse_loss(scores_loudness,targets_loudness)\n",
    "            rmse_loudness=torch.sqrt(mse_loudness)\n",
    "            sum_rmse_loudness+=float(rmse_loudness)\n",
    "\n",
    "            loss=criterion.forward(scores,targets)\n",
    "            sum_loss+=float(loss)\n",
    "\n",
    "        avg_loss= sum_loss/n_test\n",
    "        avg_rmse=sum_rmse/n_test\n",
    "        avg_rmse_note_density=sum_rmse_note_density/n_test\n",
    "        avg_rmse_loudness= sum_rmse_loudness/n_test\n",
    "        \n",
    "    model.train()\n",
    "    return avg_loss,avg_rmse,avg_rmse_note_density,avg_rmse_loudness\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "Epoch: 1\n",
      "Avg train loss: 13.056431870711478\n",
      "Avg train RMSE: 3.61013358517697\n",
      "Avg train RMSE (Note Density): 5.104191980863872\n",
      "Avg train RMSE (Loudness): 0.1152737936691234\n",
      "Avg val loss: 11.976386070251465\n",
      "Avg val RMSE: 3.4578682581583657\n",
      "Avg val RMSE (Note Density): 4.88892126083374\n",
      "Avg val RMSE (Loudness): 0.10983110467592876\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch: 2\n",
      "Avg train loss: 12.029987736752158\n",
      "Avg train RMSE: 3.4647333245528373\n",
      "Avg train RMSE (Note Density): 4.8990703632957056\n",
      "Avg train RMSE (Loudness): 0.0884090277709459\n",
      "Avg val loss: 10.947323163350424\n",
      "Avg val RMSE: 3.3058844407399497\n",
      "Avg val RMSE (Note Density): 4.674550215403239\n",
      "Avg val RMSE (Loudness): 0.07952139526605606\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch: 3\n",
      "Avg train loss: 11.648379024706388\n",
      "Avg train RMSE: 3.4115755558013916\n",
      "Avg train RMSE (Note Density): 4.823939549295526\n",
      "Avg train RMSE (Loudness): 0.08522150861589532\n",
      "Avg val loss: 10.912580172220865\n",
      "Avg val RMSE: 3.3032607237497964\n",
      "Avg val RMSE (Note Density): 4.670818169911702\n",
      "Avg val RMSE (Loudness): 0.08056721339623134\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch: 4\n",
      "Avg train loss: 11.435653786910208\n",
      "Avg train RMSE: 3.3781799768146716\n",
      "Avg train RMSE (Note Density): 4.776694097016987\n",
      "Avg train RMSE (Loudness): 0.08579380418124952\n",
      "Avg val loss: 10.898813565572103\n",
      "Avg val RMSE: 3.3004416624704995\n",
      "Avg val RMSE (Note Density): 4.666829744974772\n",
      "Avg val RMSE (Loudness): 0.08070552597443263\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch: 5\n",
      "Avg train loss: 11.060920363978335\n",
      "Avg train RMSE: 3.322555792959113\n",
      "Avg train RMSE (Note Density): 4.6980386533235245\n",
      "Avg train RMSE (Loudness): 0.0844360119418094\n",
      "Avg val loss: 10.273640632629395\n",
      "Avg val RMSE: 3.201566696166992\n",
      "Avg val RMSE (Note Density): 4.526999791463216\n",
      "Avg val RMSE (Loudness): 0.07890521983305614\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch: 6\n",
      "Avg train loss: 10.855560503507915\n",
      "Avg train RMSE: 3.2921203186637475\n",
      "Avg train RMSE (Note Density): 4.6550420710915015\n",
      "Avg train RMSE (Loudness): 0.08160567205203206\n",
      "Avg val loss: 10.880633036295572\n",
      "Avg val RMSE: 3.293411652247111\n",
      "Avg val RMSE (Note Density): 4.65699561436971\n",
      "Avg val RMSE (Loudness): 0.07367219527562459\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch: 7\n",
      "Avg train loss: 10.807556051956979\n",
      "Avg train RMSE: 3.28399207717494\n",
      "Avg train RMSE (Note Density): 4.643313257317794\n",
      "Avg train RMSE (Loudness): 0.09371689628613622\n",
      "Avg val loss: 10.778395016988119\n",
      "Avg val RMSE: 3.2803805669148765\n",
      "Avg val RMSE (Note Density): 4.638393084208171\n",
      "Avg val RMSE (Loudness): 0.084160216152668\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch: 8\n",
      "Avg train loss: 10.72187288183915\n",
      "Avg train RMSE: 3.2712668870624744\n",
      "Avg train RMSE (Note Density): 4.625491694400185\n",
      "Avg train RMSE (Loudness): 0.08464003393524572\n",
      "Avg val loss: 10.11983585357666\n",
      "Avg val RMSE: 3.1765825748443604\n",
      "Avg val RMSE (Note Density): 4.4917683601379395\n",
      "Avg val RMSE (Loudness): 0.07317248483498891\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch: 9\n",
      "Avg train loss: 10.500733275162546\n",
      "Avg train RMSE: 3.237155839016563\n",
      "Avg train RMSE (Note Density): 4.577282704805073\n",
      "Avg train RMSE (Loudness): 0.08251075093683444\n",
      "Avg val loss: 10.901999473571777\n",
      "Avg val RMSE: 3.2920997937520347\n",
      "Avg val RMSE (Note Density): 4.655091603597005\n",
      "Avg val RMSE (Loudness): 0.07679985463619232\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch: 10\n",
      "Avg train loss: 10.375252271953382\n",
      "Avg train RMSE: 3.219066569679662\n",
      "Avg train RMSE (Note Density): 4.551706891310842\n",
      "Avg train RMSE (Loudness): 0.08191363866391935\n",
      "Avg val loss: 10.47368303934733\n",
      "Avg val RMSE: 3.2360283533732095\n",
      "Avg val RMSE (Note Density): 4.575758616129558\n",
      "Avg val RMSE (Loudness): 0.07853259642918904\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch: 11\n",
      "Avg train loss: 10.22923509698165\n",
      "Avg train RMSE: 3.1944753245303503\n",
      "Avg train RMSE (Note Density): 4.516903450614528\n",
      "Avg train RMSE (Loudness): 0.0829413392041859\n",
      "Avg val loss: 10.657625834147135\n",
      "Avg val RMSE: 3.2633674144744873\n",
      "Avg val RMSE (Note Density): 4.61443567276001\n",
      "Avg val RMSE (Loudness): 0.07815920064846675\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch: 12\n",
      "Avg train loss: 9.979383920368395\n",
      "Avg train RMSE: 3.154901517064948\n",
      "Avg train RMSE (Note Density): 4.460724629853901\n",
      "Avg train RMSE (Loudness): 0.09307731178246047\n",
      "Avg val loss: 9.984448750813803\n",
      "Avg val RMSE: 3.1565069357554116\n",
      "Avg val RMSE (Note Density): 4.4632720947265625\n",
      "Avg val RMSE (Loudness): 0.07837957019607227\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch: 13\n",
      "Avg train loss: 9.796258374264365\n",
      "Avg train RMSE: 3.1267070393813285\n",
      "Avg train RMSE (Note Density): 4.420881396845767\n",
      "Avg train RMSE (Loudness): 0.0913485135687025\n",
      "Avg val loss: 10.115456581115723\n",
      "Avg val RMSE: 3.1787776152292886\n",
      "Avg val RMSE (Note Density): 4.4946010907491045\n",
      "Avg val RMSE (Loudness): 0.0883609727025032\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch: 14\n",
      "Avg train loss: 9.62929273906507\n",
      "Avg train RMSE: 3.0997996581228158\n",
      "Avg train RMSE (Note Density): 4.382875492698268\n",
      "Avg train RMSE (Loudness): 0.08884652311864652\n",
      "Avg val loss: 10.458250999450684\n",
      "Avg val RMSE: 3.2338240146636963\n",
      "Avg val RMSE (Note Density): 4.572526772816976\n",
      "Avg val RMSE (Loudness): 0.08501427869002025\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch: 15\n",
      "Avg train loss: 9.606060555106716\n",
      "Avg train RMSE: 3.0964086557689465\n",
      "Avg train RMSE (Note Density): 4.378197168049059\n",
      "Avg train RMSE (Loudness): 0.08254378210557134\n",
      "Avg val loss: 10.362822850545248\n",
      "Avg val RMSE: 3.2190850575764975\n",
      "Avg val RMSE (Note Density): 4.55185079574585\n",
      "Avg val RMSE (Loudness): 0.07517491032679875\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "=========================\n",
      "Best RMSE score vs epoch 11 : 3.1565069357554116\n",
      "Best loss score vs epoch 11 : 9.984448750813803\n",
      "=========================\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "# Tracking best evaluation accruracy\n",
    "best_eval_rmse=float(\"inf\")\n",
    "best_eval_rmse_epoch=-1\n",
    "best_eval_loss=float(\"inf\")\n",
    "best_eval_loss_epoch=-1\n",
    "\n",
    "# train\n",
    "losses=[]\n",
    "\n",
    "# yaha epochs chal rahe hai training ho rahai hai, avg loss,rmse sab calculte ho rah ahai\n",
    "for epoch in range(epochs):\n",
    "    # trains for 1 epoch\n",
    "    train_epoch(epoch,train_loader,criterion,optimizer,losses)\n",
    "\n",
    "    train_loss,train_rmse,train_rmse_note_density,train_rmse_loudness=eval_model(model,train_loader,criterion)\n",
    "    eval_loss,eval_rmse,eval_rmse_note_density,eval_rmse_loudness=eval_model(model,val_loader,criterion)\n",
    "\n",
    "    print(SEPERATOR)\n",
    "    print(\"Epoch:\", epoch+1)\n",
    "    print(\"Avg train loss:\", train_loss)\n",
    "    print(\"Avg train RMSE:\", train_rmse)\n",
    "    print(\"Avg train RMSE (Note Density):\", train_rmse_note_density)\n",
    "    print(\"Avg train RMSE (Loudness):\", train_rmse_loudness)\n",
    "    \n",
    "    print(\"Avg val loss:\", eval_loss)\n",
    "    print(\"Avg val RMSE:\", eval_rmse)\n",
    "    print(\"Avg val RMSE (Note Density):\", eval_rmse_note_density)\n",
    "    print(\"Avg val RMSE (Loudness):\", eval_rmse_loudness)\n",
    "\n",
    "    print(SEPERATOR)\n",
    "    print(\"\")\n",
    "\n",
    "    if eval_rmse<best_eval_rmse:\n",
    "        best_eval_rmse=eval_rmse\n",
    "        best_eval_rmse_epoch=epoch\n",
    "\n",
    "    if eval_loss<best_eval_loss:\n",
    "        best_eval_loss=eval_loss\n",
    "        best_eval_loss_epoch=epoch\n",
    "\n",
    "print(SEPERATOR)\n",
    "print(SEPERATOR)\n",
    "print(\"Best RMSE score vs epoch\",best_eval_rmse_epoch,\":\",best_eval_rmse)\n",
    "print(\"Best loss score vs epoch\",best_eval_loss_epoch,\":\",best_eval_loss)\n",
    "print(SEPERATOR)\n",
    "print(SEPERATOR)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "Avg test loss: 11.504242579142252\n",
      "Avg test RMSE: 3.382254441579183\n",
      "Avg test RMSE (Note Density): 4.782306512196858\n",
      "Avg test RMSE (Loudness): 0.09370045860608418\n",
      "=========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing \n",
    "test_loss,test_rmse,test_rmse_note_density,test_rmse_loudness=eval_model(model,test_loader,criterion)\n",
    "\n",
    "print(SEPERATOR)\n",
    "print(\"Avg test loss:\", test_loss)\n",
    "print(\"Avg test RMSE:\", test_rmse)\n",
    "print(\"Avg test RMSE (Note Density):\", test_rmse_note_density)\n",
    "print(\"Avg test RMSE (Loudness):\", test_rmse_loudness)\n",
    "\n",
    "print(SEPERATOR)\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
